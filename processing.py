import logging
from typing import Tuple, List, Dict

from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim
import transformers
import torch


def get_related_knowledge(question: str, knowledge_base: Dict[str, List[float]],
                          threshold: float = 0.7) -> str:
    """Finds the most relevant knowledge chunk based on cosine similarity to the input question.

    Args:
        question (str): The question for which related knowledge is needed.
        knowledge_base (Dict[str, List[float]]): The knowledge base, with text as keys and embeddings as values.
        threshold (float): The minimum similarity score to consider as a sufficient match.

    Returns:
        str: The most relevant knowledge chunk
    """

    model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)
    question_embedding = model.encode(question)
    related_knowledge = get_similar_chunks(knowledge_base, question_embedding)

    if related_knowledge[0][1] < threshold:
        logging.error("There was no sufficient information in the directive.")

    return related_knowledge[0][0]


def get_similar_chunks(chunks_vectors: Dict[str, List[float]],
                       input_embedding: List[float]) -> List[Tuple[str, float]]:
    """
    Calculates the cosine similarity between the input embedding and each chunk's embedding,
    and returns the chunks sorted by their similarity to the input embedding.

    Parameters:
    - chunks_vectors (List[Tuple[List[float], str]]): A list of tuples containing the embedding
      and corresponding text chunk.
    - input_embedding (List[float]): The embedding of the input text to compare against the chunks.

    Returns:
    - List[Tuple[str, float]]: A list of tuples where each tuple contains a text chunk and its
      cosine similarity score, sorted in descending order of similarity.
    """
    chunks_vectors = list(zip(chunks_vectors.keys(), chunks_vectors.values()))
    cosines = []
    for text, embedding in chunks_vectors:
        cosine_similarity = cos_sim(embedding, input_embedding).item()
        cosines.append((text, cosine_similarity))

    sorted_cosines = sorted(cosines, key=lambda x: x[1], reverse=True)

    return sorted_cosines


class LlamaQAModel:
    def __init__(self, model_name: str, max_length: int = 1000):
        """
        Initialize the LlamaQAModel by loading the LLaMA model using transformers pipeline.

        Args:
            model_name (str): The name of the LLaMA model to load.
            max_length (int): Maximum length of the generated response.
        """
        self.model_name = model_name
        self.max_length = max_length
        self.model = transformers.pipeline("text-generation",
                                           model=model_name,
                                           model_kwargs={"torch_dtype": torch.bfloat16},
                                           device_map="auto")
        logging.info(f"Model '{model_name}' loaded successfully.")

    def _structure_prompt(self, knowledge_base: str, question: str) -> str:
        """
        Structure the prompt by embedding the knowledge base and the question.

        Args:
            knowledge_base (str): The knowledge base text.
            question (str): The question to answer.

        Returns:
            str: The structured prompt for the model.
        """

        prompt = f"""[INSTRUCTIONS]
        Use the following information in the [KNOWLEDGE BASE] to answer the question as accurately as possible.
        If there is no answer in the [KNOWLEDGE BASE] return: 'There was no sufficient information in the directive.'.
        
        [KNOWLEDGE BASE]
        {knowledge_base}
        
        [QUESTION]
        {question}
        
        [ANSWER]
        """

        return prompt

    def ask_question(self, knowledge_base: str, question: str) -> str:
        """
        Generate an answer to the question based on the knowledge base.

        Args:
            knowledge_base (str): The knowledge base text.
            question (str): The question to answer.

        Returns:
            str: The answer generated by the model.
        """

        prompt = self._structure_prompt(knowledge_base, question)
        response = self.model(prompt, max_length=self.max_length, do_sample=True)
        answer = response[0]['generated_text'].split("[ANSWER]")[1].strip()

        return answer
